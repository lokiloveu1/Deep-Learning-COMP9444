{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# General Parameters\n",
    "# -- DO NOT MODIFY --\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "EPISODE = 200000  # Episode limitation\n",
    "STEP = 200  # Step limitation in an episode\n",
    "TEST = 10  # The number of tests to run every TEST_FREQUENCY episodes\n",
    "TEST_FREQUENCY = 10  # Num episodes to run before visualizing test accuracy\n",
    "\n",
    "# TODO: HyperParameters\n",
    "# GAMMA = 0.99 # discount factor\n",
    "# INITIAL_EPSILON = 0.7 # starting value of epsilon ### Kind of low.\n",
    "# FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "# EPSILON_DECAY_STEPS = 500 # decay period\n",
    "# SIZE_BUFFER = 10000 # size of the buffer\n",
    "# BATCH_SIZE = 64 # batch size\n",
    "\n",
    "GAMMA = 0.99 # discount factor\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon ### Kind of low.\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "EPSILON_DECAY_STEPS = 1000 # decay period\n",
    "SIZE_BUFFER = 10000 # size of the buffer\n",
    "BATCH_SIZE = 64 # batch size\n",
    "# # Create environment\n",
    "# -- DO NOT MODIFY --\n",
    "env = gym.make(ENV_NAME)\n",
    "epsilon = INITIAL_EPSILON\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.n\n",
    "\n",
    "# Placeholders\n",
    "# -- DO NOT MODIFY --\n",
    "state_in = tf.placeholder(\"float\", [None, STATE_DIM])\n",
    "action_in = tf.placeholder(\"float\", [None, ACTION_DIM])\n",
    "target_in = tf.placeholder(\"float\", [None])\n",
    "\n",
    "\n",
    "# w1 = tf.Variable(tf.truncated_normal([STATE_DIM,20]))\n",
    "# b1 = tf.Variable(tf.constant(0.01, shape = [20]))\n",
    "# w2 = tf.Variable(tf.truncated_normal([20,ACTION_DIM]))\n",
    "# b2 = tf.Variable(tf.constant(0.01, shape = [ACTION_DIM]))\n",
    "# h_layer = tf.nn.relu(tf.matmul(state_in, w1)+b1)\n",
    "# q_values = tf.matmul(h_layer,w2)+b2\n",
    "# # TODO: Define Network Graph\n",
    "def my_network(state_in, ACTION_DIM):\n",
    "    hidden_layer = tf.layers.dense(\n",
    "                state_in,\n",
    "                128,\n",
    "                kernel_initializer=tf.truncated_normal_initializer(),\n",
    "                bias_initializer=tf.constant_initializer(0.01),\n",
    "                activation=tf.nn.relu\n",
    "            )\n",
    "    output_layer = tf.layers.dense(\n",
    "                hidden_layer,\n",
    "                ACTION_DIM,\n",
    "                kernel_initializer=tf.truncated_normal_initializer(),\n",
    "                bias_initializer=tf.constant_initializer(0.01),\n",
    "            )\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "# TODO: Network outputs\n",
    "q_values = my_network(state_in,ACTION_DIM)\n",
    "q_action = tf.reduce_sum(tf.multiply(q_values,action_in),reduction_indices=1)\n",
    "\n",
    "# TODO: Loss/Optimizer Definition\n",
    "loss = tf.reduce_mean(tf.square(target_in - q_action))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# loss = tf.reduce_mean(tf.square(target_in - q_action))\n",
    "# optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "replay_buffer = deque()\n",
    "\n",
    "# Start session - Tensorflow housekeeping\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# -- DO NOT MODIFY ---\n",
    "def explore(state, epsilon):\n",
    "    \"\"\"\n",
    "    Exploration function: given a state and an epsilon value,\n",
    "    and assuming the network has already been defined, decide which action to\n",
    "    take using e-greedy exploration based on the current q-value estimates.\n",
    "    \"\"\"\n",
    "    Q_estimates = q_values.eval(feed_dict={\n",
    "        state_in: [state]\n",
    "    })\n",
    "    if random.random() <= epsilon:\n",
    "        action = random.randint(0, ACTION_DIM - 1)\n",
    "    else:\n",
    "        action = np.argmax(Q_estimates)\n",
    "\n",
    "    one_hot_action = np.zeros(ACTION_DIM)\n",
    "    one_hot_action[action] = 1\n",
    "\n",
    "    return one_hot_action\n",
    "\n",
    "\n",
    "# Main learning loop\n",
    "for episode in range(EPISODE):\n",
    "\n",
    "    # initialize task\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    # Update epsilon once per episode\n",
    "    epsilon -= epsilon / EPSILON_DECAY_STEPS\n",
    "\n",
    "    # Move through env according to e-greedy policy\n",
    "    for step in range(STEP):\n",
    "        action = explore(state, epsilon)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(np.argmax(action))\n",
    "\n",
    "\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        count = 1\n",
    "\n",
    "        if len(replay_buffer) > SIZE_BUFFER:\n",
    "            replay_buffer.popleft()\n",
    "            count+=1\n",
    "\n",
    "        if len(replay_buffer) > BATCH_SIZE:\n",
    "\n",
    "            minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            state_batch = [x[0] for x in minibatch]\n",
    "            action_batch = [x[1] for x in minibatch]\n",
    "            reward_batch = [x[2] for x in minibatch]\n",
    "            next_state_batch = [x[3] for x in minibatch]\n",
    "\n",
    "            #print(minibatch)\n",
    "\n",
    "            target_batch = []\n",
    "\n",
    "            nextstate_q_values = q_values.eval(feed_dict={\n",
    "                state_in: next_state_batch\n",
    "            })\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "\n",
    "                if minibatch[i][4]:\n",
    "                    target = reward_batch[i]-480   #punish the model if done(decrease the reward when the pole fall down)\n",
    "                else:\n",
    "                    target = GAMMA * np.max(nextstate_q_values[i]) + reward_batch[i]\n",
    "\n",
    "                target_batch.append(target)\n",
    "                \n",
    "            # Do one training step\n",
    "            session.run([optimizer], feed_dict={\n",
    "                target_in: target_batch,\n",
    "                action_in: action_batch,\n",
    "                state_in: state_batch\n",
    "            })\n",
    "\n",
    "        # Update\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Test and view sample runs - can disable render to save time\n",
    "    # -- DO NOT MODIFY --\n",
    "    if (episode % TEST_FREQUENCY == 0 and episode != 0):\n",
    "        total_reward = 0\n",
    "        for i in range(TEST):\n",
    "            state = env.reset()\n",
    "            for j in range(STEP):\n",
    "                #env.render()\n",
    "                action = np.argmax(q_values.eval(feed_dict={\n",
    "                    state_in: [state]\n",
    "                }))\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "        ave_reward = total_reward / TEST\n",
    "        print('episode:', episode, 'epsilon:', epsilon, 'Evaluation '\n",
    "                                                        'Average Reward:', ave_reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
